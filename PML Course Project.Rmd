---
title: "PML Course Project"
author: "Aaron Ran An"
date: "September 23, 2015"
output: pdf_document
---

## Executive Summary

This is the course project for JHU's Coursera Course, Practical Machine Learning. The data for this project come from this source: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har).

## Learning Objective & Design

The object of this project is to predict the manner in which the people do the exercise, which is labeled as the "classe" in the data set. The machine learning techinques and process include data preparation, features selections, cross validation, fitting models and validating the model using out of sample error and confusion matrix for classification. 

## Data Prepration

First step is to read the training and test set into R. 

```{r eval = F}

train <- read.csv("../Proj_Data/pml-training.csv")

test <- read.csv("../Proj_Data/pml-testing.csv")

```

```{r}

# Load necessary pacakge

library(caret)

```

Inspect the dimension of the dataset and the class distribution of the target response.

```{r}

table(train$classe)

dim(train)

```

After initial inspection, there are many columns that either doesn't contribute much to our prediction (eg. timestamps) or are populated with too many NA values which will heavily reduce our training sample size (most alogrithms we work with only apply on complete cases). So next step needs to detect and remove them from our training set. 

```{r eval=F}

Null_Counter <- apply(train, 2, function(x) length(which(x == "" | is.na(x)))/length(x)) 

Null_Name <- colnames(train)[Null_Counter >= 0.9] # drop the columns with more than 90% null. Nulls appear as "" or NA. 

In_Check <- names(train) %in% Null_Name; 

train <- train[, !In_Check]; test <- test[, !In_Check] # apply this selection on both training and test sets. 

train <- train[, c(-1:-6)]; dim(train) # Check the final training data set.

test <- test[, c(-1:-6)]; dim(test) # apply the same preparation process on test set. 

```

## Data Partition 

A standard step of spliting data into training (70%) and validation (30%) set (which is called testData here).

```{r eval=F}

set.seed(12345)
inTrain <- createDataPartition(train$classe, p=0.70, list=F)
trainData <- train[inTrain, ]
testData <- train[-inTrain, ]

```

## Modeling with Random Forrest

My machine learning approach is random forest with 5 folds cross validation and 500 trees.  
```{r}

controlRf <- trainControl(method="cv", 5)

fit_rf <- train(classe ~ ., data=trainData, method="rf", trControl=controlRf, ntree=250)

fit_rf

varImp(fit_rf)

```

Random forest performed well on the training set, in sample accuracy is about 99.68%. Also the variable importance table listed the most important 20 features in our data from scale of 0 - 100. This would give you a sense of which variables are more important than classify excercise. 

Next we evaluate the model performance on the validation set and calculate out of sample error rate. 

```{r}

rf_hat <- predict(fit_rf, testData)

confusionMatrix(testData$classe, rf_hat)

accuracy <- postResample(rf_hat, testData$classe); accuracy

error_rate <- 1 - as.numeric(accuracy[1])

```

The test set accuracy is about 0.9962617, therefore the out of sample error rate is `r 1- as.numeric(accuracy[1])`. 

## Generate the final submission on the test set.

```{r}

prediction <- predict(fit_rf, test)
prediction

```

The model correctly classified 18 out of 20 data points in the test set. 





